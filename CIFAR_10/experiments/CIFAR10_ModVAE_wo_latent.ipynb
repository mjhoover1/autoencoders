{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CIFAR10_ModVAE_wo_latent.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP+ZBEWSw+GyA06/TwrKXU6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mjhoover1/autoencoders/blob/main/CIFAR_10/experiments/CIFAR10_ModVAE_wo_latent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gR9xtPGdZ5S-"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IrO11fiWSG5X"
      },
      "outputs": [],
      "source": [
        "import sklearn\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)\n",
        "import os\n",
        "from keras.layers import *\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Dx8Q7FXvaFd-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_and_train_model(normals, abnormals, results_dir):\n",
        "    (x_train_0, y_train_0), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
        "\n",
        "    x_train_0 = x_train_0.astype(np.float32) / 255\n",
        "    x_test = x_test.astype(np.float32) / 255\n",
        "\n",
        "    train_size = x_train_0.shape[0] * 9 // 10\n",
        "\n",
        "    x_train, x_valid, y_train, y_valid = train_test_split(x_train_0, y_train_0, train_size = train_size)\n",
        "\n",
        "    normal_data = x_train[np.isin(y_train, normals).flatten()]    # Normal training data (Normal digits)\n",
        "    normal_labels = y_train[np.isin(y_train, normals).flatten()]  \n",
        "\n",
        "    valid_data = x_valid[np.isin(y_valid, abnormals).flatten() | np.isin(y_valid, normals).flatten()]    # Validation data (both normal digits and the abnormal digit)\n",
        "    valid_labels = y_valid[np.isin(y_valid, abnormals).flatten() | np.isin(y_valid, normals).flatten()]\n",
        "\n",
        "    test_data = x_test[np.isin(y_test, abnormals).flatten() | np.isin(y_test, normals).flatten()]   # Test data (both normal digits and the abnormal digit)\n",
        "    test_labels = y_test[np.isin(y_test, abnormals).flatten() | np.isin(y_test, normals).flatten()]\n",
        "\n",
        "    test_labels_T_F = np.where(np.isin(test_labels, normals).flatten(), True, False) \n",
        "    # Array of T and F, T where test digits are normal and F where test digits are abnormal\n",
        "\n",
        "    valid_labels_T_F = np.where(np.isin(valid_labels, normals).flatten(), True, False) \n",
        "    # Array of T and F, T where test digits are normal and F where test digits are abnormal\n",
        "\n",
        "    normal_data.shape, normal_labels.shape, valid_data.shape, valid_labels.shape, test_data.shape, test_labels.shape\n",
        "\n",
        "    normal_test_data = test_data[np.isin(test_labels, normals).flatten()]   # The normal digits in the test data\n",
        "    abnormal_test_data = test_data[np.isin(test_labels, abnormals).flatten()]                          # The abnormal digits in the test data\n",
        "    normal_test_labels = test_labels[np.isin(test_labels, normals).flatten()]   # Their labels\n",
        "    abnormal_test_labels = test_labels[np.isin(test_labels, abnormals).flatten()]                          # Their labels\n",
        "\n",
        "    normal_test_data.shape, abnormal_test_data.shape\n",
        "\n",
        "    normal_valid_data = valid_data[np.isin(valid_labels, normals).flatten()]   # The normal digits in the valid data\n",
        "    abnormal_valid_data = valid_data[np.isin(valid_labels, abnormals).flatten()]                           # The abnormal digits in the valid data\n",
        "    normal_valid_labels = valid_labels[np.isin(valid_labels, normals).flatten()]   # Their labels\n",
        "    abnormal_valid_labels = valid_labels[np.isin(valid_labels, abnormals).flatten()]                           # Their labels\n",
        "\n",
        "    normal_valid_data.shape, abnormal_valid_data.shape\n",
        "\n",
        "    \"\"\"### **Building and training the network** \"\"\"\n",
        "    K = keras.backend\n",
        "\n",
        "    # For details please see Geron's book. Uses the reparametrization trick to do stochastic \n",
        "    # sampling from the MVN distribution, while allowing the 2 parallel layers containing the \n",
        "    # means and stds of the MVN distribution for each dimension to be trained via \n",
        "    # backpropogation of the error signal.\n",
        "    class Sampling(keras.layers.Layer):\n",
        "        def call(self, inputs):\n",
        "            mean, log_var = inputs\n",
        "            return K.random_normal(tf.shape(log_var)) * K.exp(log_var / 2) + mean \n",
        "\n",
        "    \n",
        "    # For details please see Geron's book. \n",
        "    codings_size = 16   # The number of dimensions of the MVN distribution in the sampling layer\n",
        "\n",
        "    # inputs = keras.layers.Input(shape=[32, 32, 3])\n",
        "    # z = keras.layers.Flatten()(inputs)\n",
        "    # z = keras.layers.Dense(256, activation=\"selu\")(z)\n",
        "    # z = keras.layers.Dense(128, activation=\"selu\")(z)\n",
        "    # z = keras.layers.Dense(64, activation=\"selu\")(z)\n",
        "\n",
        "    inputs = keras.layers.Input(shape=[32, 32, 3])\n",
        "    # z = keras.layers.Flatten()(inputs)\n",
        "    # z = keras.layers.Dense(128, activation=\"selu\")(z)\n",
        "    #z = keras.layers.Reshape((257, 97, 1)), \n",
        "    z = Conv2D(64, (3, 3), padding='same')(inputs)\n",
        "    z = Conv2D(64, (3, 3), padding='same')(z)\n",
        "    # z = BatchNormalization(z)\n",
        "    z = Activation('relu')(z)\n",
        "    z = MaxPooling2D((2, 2), padding='same')(z)\n",
        "    z = Conv2D(32, (3, 3), padding='same')(z)\n",
        "    z = Conv2D(32, (3, 3), padding='same')(z)\n",
        "    # z = BatchNormalization(z)\n",
        "    z = Activation('relu')(z)\n",
        "    z = MaxPooling2D((2, 2), padding='same')(z)\n",
        "    z = keras.layers.Flatten()(z)\n",
        "\n",
        "    # Parallel layers at the end of the encoder for means \n",
        "    # and standard deviations of the Multivariate Normal (MVN) distribution \n",
        "    # in the dimensions of the coding size (here 32). \n",
        "    codings_mean = keras.layers.Dense(codings_size)(z)\n",
        "    codings_log_var = keras.layers.Dense(codings_size)(z)\n",
        "    \n",
        "    # Sampling layer at the end of the encoder\n",
        "    codings = Sampling()([codings_mean, codings_log_var])\n",
        "    variational_encoder = keras.models.Model(\n",
        "        inputs=[inputs], outputs=[codings_mean, codings_log_var, codings])\n",
        "\n",
        "    # tf.keras.layers.InputLayer(input_shape=(latent_dim,)),\n",
        "    #            tf.keras.layers.Dense(units=7*7*32, activation=tf.nn.relu),\n",
        "    #            tf.keras.layers.Reshape(target_shape=(7, 7, 32))\n",
        "\n",
        "    decoder_inputs = keras.layers.Input(shape=[codings_size])\n",
        "\n",
        "    x = keras.layers.Dense(units=8*8*32, activation=\"relu\")(decoder_inputs)\n",
        "    x = keras.layers.Reshape(target_shape=(8, 8, 32))(x)\n",
        "    x = Conv2D(32, (3, 3), padding='same')(x)\n",
        "    x = Conv2D(32, (3, 3), padding='same')(x)\n",
        "    # x = BatchNormalization(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = UpSampling2D((2,2))(x)\n",
        "\n",
        "    x = Conv2D(64, (3, 3), padding='same')(x)\n",
        "    x = Conv2D(64, (3, 3), padding='same')(x)\n",
        "    # x = BatchNormalization(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = UpSampling2D((2,2))(x)\n",
        "    # x = keras.layers.Dense(128, activation=\"selu\")(x)\n",
        "    # x = keras.layers.Dense(32 * 32 * 3, activation=\"sigmoid\")(x)\n",
        "    x = Conv2D(3, (3, 3), padding='same')(x)\n",
        "    x = Conv2D(3, (3, 3), padding='same')(x)\n",
        "    # x = BatchNormalization(x)\n",
        "    x = Activation('sigmoid')(x)\n",
        "    outputs = keras.layers.Reshape([32, 32, 3])(x)\n",
        "\n",
        "    # x = keras.layers.Dense(64, activation=\"selu\")(decoder_inputs)\n",
        "    # x = keras.layers.Dense(128, activation=\"selu\")(x)\n",
        "    # x = keras.layers.Dense(256, activation=\"selu\")(x)\n",
        "    # x = keras.layers.Dense(32 * 32 * 3, activation=\"sigmoid\")(x)\n",
        "    # outputs = keras.layers.Reshape([32, 32, 3])(x)\n",
        "    variational_decoder = keras.models.Model(inputs=[decoder_inputs], outputs=[outputs])\n",
        "\n",
        "    _, _, codings = variational_encoder(inputs)\n",
        "    reconstructions = variational_decoder(codings)\n",
        "    variational_ae = keras.models.Model(inputs=[inputs], outputs=[reconstructions])\n",
        "\n",
        "    # The latent loss function\n",
        "    # latent_loss = -0.5 * K.sum(\n",
        "    #    1 + codings_log_var - K.exp(codings_log_var) - K.square(codings_mean),\n",
        "    #    axis=-1)\n",
        "\n",
        "    # Add the latent loss to the reconstruction loss\n",
        "    # variational_ae.add_loss(K.mean(latent_loss) / 784.)\n",
        "\n",
        "    # For the reconstruction loss binary cross-entropy loss is used. \n",
        "    # For details please see Chapter 17 of Geron's book (Stacked AE and VAE sections) \n",
        "    variational_ae.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\")\n",
        "\n",
        "    checkpoint_cb = keras.callbacks.ModelCheckpoint(\"wo_latent_VAE_model\", monitor=\"val_loss\", save_best_only=True)\n",
        "\n",
        "    history = variational_ae.fit(normal_data, normal_data, epochs=100, batch_size=128, callbacks=[checkpoint_cb],\n",
        "                                validation_data=(normal_valid_data, normal_valid_data), shuffle=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "    plt.plot(history.history[\"loss\"], label=\"Training Loss\")\n",
        "    plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\n",
        "    plt.legend()\n",
        "    plt.savefig(os.path.join(results_dir, 'loss_history.png'))\n",
        "    plt.close()\n",
        "\n",
        "    model = variational_ae\n",
        "    model.summary(expand_nested=True, show_trainable=True)\n",
        "\n",
        "    model_encoder = variational_encoder\n",
        "    # model_encoder.summary(expand_nested=True, show_trainable=True)\n",
        "\n",
        "    model_decoder = variational_decoder\n",
        "    # model_decoder.summary(expand_nested=True, show_trainable=True)\n",
        "\n",
        "    model_layers = np.array(model.layers)\n",
        "    n_layers = model_layers.shape[0] \n",
        "    # np.concatenate((np.arange(n_layers).reshape(n_layers,1), model_layers.reshape(n_layers,1)), axis = 1)\n",
        "\n",
        "    \"\"\"### **The original and reconstructed images for the first 30 instances of the normal training data, validation data, normal validation data, abnormal validation data, test data, normal test data, and abnormal test data**\"\"\"\n",
        "\n",
        "    def plot_image(image):\n",
        "        plt.imshow(image, cmap=\"binary\")\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "    def show_reconstructions(model, images, n_images=5):\n",
        "        reconstructions = model.predict(images[:n_images])\n",
        "        fig = plt.figure(figsize=(n_images * 1.5, 3))\n",
        "        for image_index in range(n_images):\n",
        "            plt.subplot(2, n_images, 1 + image_index)\n",
        "            plot_image(images[image_index])\n",
        "            plt.subplot(2, n_images, 1 + n_images + image_index)\n",
        "            plot_image(reconstructions[image_index])\n",
        "\n",
        "    show_reconstructions(variational_ae, normal_data, 30)\n",
        "    plt.show()\n",
        "    plt.savefig(os.path.join(results_dir, 'reconstructions_normal.png'))\n",
        "    plt.close()\n",
        "\n",
        "    show_reconstructions(variational_ae, valid_data, 30)\n",
        "    plt.show()\n",
        "    plt.savefig(os.path.join(results_dir, 'reconstructions_valid.png'))\n",
        "    plt.close()\n",
        "\n",
        "    show_reconstructions(variational_ae, normal_valid_data, 30)\n",
        "    plt.show()\n",
        "    plt.savefig(os.path.join(results_dir, 'reconstructions_normal_valid.png'))\n",
        "    plt.close()\n",
        "\n",
        "    show_reconstructions(variational_ae, abnormal_valid_data, 30)\n",
        "    plt.show()\n",
        "    plt.savefig(os.path.join(results_dir, 'reconstructions_abnormal_valid.png'))\n",
        "    plt.close()\n",
        "\n",
        "    show_reconstructions(variational_ae, test_data, 30)\n",
        "    plt.show()\n",
        "    plt.savefig(os.path.join(results_dir, 'reconstructions_test.png'))\n",
        "    plt.close()\n",
        "\n",
        "    show_reconstructions(variational_ae, normal_test_data, 30)\n",
        "    plt.show()\n",
        "    plt.savefig(os.path.join(results_dir, 'reconstructions_normal_test.png'))\n",
        "    plt.close()\n",
        "\n",
        "    show_reconstructions(variational_ae, abnormal_test_data, 30)\n",
        "    plt.show()\n",
        "    plt.savefig(os.path.join(results_dir, 'reconstructions_abnormal_test.png'))\n",
        "    plt.close()\n",
        "\n",
        "    \"\"\"**1-Dim plot of pixels of the first normal test data**\"\"\"\n",
        "\n",
        "    reconstructions_nl_test = variational_ae.predict(normal_test_data)\n",
        "\n",
        "    plt.figure(figsize=(25,7))\n",
        "    plt.plot(normal_test_data[0].ravel(), 'r')\n",
        "    plt.plot(reconstructions_nl_test[0].ravel(), 'g')\n",
        "    plt.fill_between(np.arange(32*32*3), reconstructions_nl_test[0].ravel(), normal_test_data[0].ravel(), color='blue')\n",
        "    plt.legend(labels=[\"Input\", \"Reconstruction\", \"Error\"])\n",
        "    plt.show()\n",
        "    plt.savefig(os.path.join(results_dir, 'reconstruction_error_normal.png'))\n",
        "    plt.close()\n",
        "\n",
        "    \"\"\"**1-Dim plot of pixels of the first abnormal test data**\"\"\"\n",
        "\n",
        "    reconstructions_nl_test = variational_ae.predict(normal_test_data)\n",
        "    \n",
        "    plt.figure(figsize=(25,7))\n",
        "    plt.plot(normal_test_data[0].ravel(), 'r')\n",
        "    plt.plot(reconstructions_nl_test[0].ravel(), 'g')\n",
        "    plt.fill_between(np.arange(32*32*3), reconstructions_nl_test[0].ravel(), normal_test_data[0].ravel(), color='blue')\n",
        "    plt.legend(labels=[\"Input\", \"Reconstruction\", \"Error\"])\n",
        "    plt.show()\n",
        "    plt.savefig(os.path.join(results_dir, 'reconstruction_error_normal.png'))\n",
        "    plt.close()\n",
        "\n",
        "    \"\"\"### **Distributions of the reconstruction losses and the calculation of the threshold.**\n",
        "\n",
        "    **Distribution of the reconstruction losses of the normal training data**\n",
        "    \"\"\"\n",
        "    reconstructions = variational_ae.predict(normal_data)\n",
        "    train_loss = tf.keras.losses.mae(reconstructions.reshape(-1, 3072), normal_data.reshape(-1, 3072))\n",
        "    plt.figure(figsize=(10,5))\n",
        "    plt.hist(train_loss[None,:], bins=100)\n",
        "    threshold1 = np.mean(train_loss) + 2.5*np.std(train_loss)\n",
        "    plt.axvline(threshold1,c='g')\n",
        "    plt.xlabel(\"MAE reconstruction loss of the normal training data\")\n",
        "    plt.ylabel(\"No of examples\")\n",
        "    plt.show()\n",
        "    plt.savefig(os.path.join(results_dir, 'reconstruction_losses_normal.png'))\n",
        "    plt.close()\n",
        "\n",
        "    print(\"Mean: \", np.mean(train_loss))\n",
        "    print(\"Std: \", np.std(train_loss))\n",
        "\n",
        "    threshold_train_mean_2_5_std = np.mean(train_loss) + 2.5*np.std(train_loss)\n",
        "    print(\"Threshold based on the mean of the training data MAE reconstruction losses + 2.5 std: \", threshold_train_mean_2_5_std)\n",
        "\n",
        "    threshold1 = threshold_train_mean_2_5_std\n",
        "\n",
        "    \"\"\"**Distribution of the reconstruction losses of the abnormal validation data**\"\"\"\n",
        "    reconstructions = variational_ae.predict(abnormal_valid_data)\n",
        "    abn_valid_loss = tf.keras.losses.mae(reconstructions.reshape(-1,3072), abnormal_valid_data.reshape(-1,3072))\n",
        "    plt.figure(figsize=(10,5))\n",
        "    plt.hist(abn_valid_loss[None, :], bins=100)\n",
        "    threshold2 = np.mean(abn_valid_loss) - np.std(abn_valid_loss)\n",
        "    plt.axvline(threshold2,c='cyan')\n",
        "    plt.axvline(threshold1,c='g')\n",
        "    plt.xlabel(\"MAE reconstruction loss of the abnormal validation data\")\n",
        "    plt.ylabel(\"No of examples\")\n",
        "    plt.show()\n",
        "    plt.savefig(os.path.join(results_dir, 'reconstruction_loss_abnormal_validation_data.png'))\n",
        "    plt.close()\n",
        "\n",
        "    abnormal_valid_mean_loss = np.mean(abn_valid_loss)\n",
        "\n",
        "    abnormal_valid_mean_loss , np.std(abn_valid_loss)\n",
        "\n",
        "    threshold2 = abnormal_valid_mean_loss - np.std(abn_valid_loss)\n",
        "    print(\"Threshold2: \", threshold2)\n",
        "\n",
        "    \"\"\"**Distribution of the reconstruction losses of the normal validation data**\"\"\"\n",
        "\n",
        "    reconstructions = variational_ae.predict(normal_valid_data)\n",
        "    nl_valid_loss = tf.keras.losses.mae(reconstructions.reshape(-1,3072), normal_valid_data.reshape(-1,3072))\n",
        "    plt.figure(figsize=(10,5))\n",
        "    plt.hist(nl_valid_loss[None, :], bins=100)\n",
        "    threshold3 = np.mean(nl_valid_loss) + np.std(nl_valid_loss)\n",
        "    plt.axvline(threshold3, c='magenta')\n",
        "    plt.axvline(threshold2, c='cyan')\n",
        "    plt.axvline(threshold1, c='g')\n",
        "    plt.xlabel(\"MAE reconstruction loss of the normal validation data\")\n",
        "    plt.ylabel(\"No of examples\")\n",
        "    plt.show()\n",
        "\n",
        "    normal_valid_mean_loss = np.mean(nl_valid_loss)\n",
        "    normal_valid_mean_loss , np.std(nl_valid_loss)\n",
        "\n",
        "    threshold3 = normal_valid_mean_loss + np.std(nl_valid_loss)\n",
        "    print(\"Threshold3: \", threshold3)\n",
        "\n",
        "    \"\"\"**Calculation of a preliminary threshold based on (threshold2 + threshold3) / 2 = Average of (mean + std of the distribution of the reconstruction losses of the normal validation data) and (mean - std of the distribution of the reconstruction losses of the abnormal validation data)**\"\"\"\n",
        "\n",
        "    Avg_of_threshold_2_3 = (threshold2 + threshold3)/2\n",
        "    print(\"Average of threshold 2 and 3: \", Avg_of_threshold_2_3)\n",
        "\n",
        "    threshold4 = Avg_of_threshold_2_3\n",
        "\n",
        "    \"\"\"### **Calculation of the threshold that gives the best accuracy on the validation data and set this as the threshold.**\"\"\"\n",
        "\n",
        "    def predict(model, data, threshold):\n",
        "      reconstructions = model.predict(data)\n",
        "      loss = tf.keras.losses.mae(reconstructions.reshape(-1, 3072), data.reshape(-1, 3072))\n",
        "      return tf.math.less(loss, threshold)\n",
        "\n",
        "    increment = (abnormal_valid_mean_loss- normal_valid_mean_loss)/100\n",
        "    thresholds = np.arange(normal_valid_mean_loss, abnormal_valid_mean_loss, increment)\n",
        "    thrs_size = thresholds.shape[0]\n",
        "    accuracies = np.zeros(thrs_size)\n",
        "    for i in range(thrs_size):\n",
        "      preds = predict(variational_ae, valid_data, thresholds[i])\n",
        "      accuracies[i] = accuracy_score(preds, valid_labels_T_F)\n",
        "    argmax = np.argmax(accuracies)\n",
        "    valid_data_best_threshold = thresholds[argmax]\n",
        "    print(\"The best threshold based on validation data: \", valid_data_best_threshold)\n",
        "\n",
        "    thr_acc = np.zeros((thrs_size, 2))\n",
        "    thr_acc[:, 0] = thresholds\n",
        "    thr_acc[:, 1] = accuracies\n",
        "    thr_acc[argmax-2:argmax+3]\n",
        "\n",
        "    threshold5 = valid_data_best_threshold\n",
        "\n",
        "    threshold = threshold5\n",
        "\n",
        "    \"\"\"#### **Distribution of the reconstruction losses of all the validation data (normal and abnormal)**\n",
        "\n",
        "    The blue line is threshold4 (= the average of threshold3 [magenta] and threshold2 [cyan]). \n",
        "\n",
        "    The red line is the threshold that gives the best accuracy for the validation data.\n",
        "    \"\"\"\n",
        "\n",
        "    reconstructions = variational_ae.predict(valid_data)\n",
        "    valid_loss = tf.keras.losses.mae(reconstructions.reshape(-1,3072), valid_data.reshape(-1,3072))\n",
        "\n",
        "    \n",
        "    reconstructions = variational_ae.predict(valid_data)\n",
        "    valid_loss = tf.keras.losses.mae(reconstructions.reshape(-1,3072), valid_data.reshape(-1,3072))\n",
        "    plt.figure(figsize=(20,5))\n",
        "    plt.hist(valid_loss[None, :], bins=100)\n",
        "    plt.axvline(threshold, c='r')\n",
        "    plt.axvline(threshold4, c='b')\n",
        "    plt.axvline(threshold2, c='cyan')\n",
        "    plt.axvline(threshold3, c='magenta')\n",
        "    plt.axvline(threshold1, c='green')\n",
        "    plt.xlabel(\"MAE reconstruction loss of the validation data\")\n",
        "    plt.ylabel(\"No of examples\")\n",
        "    plt.show()\n",
        "    plt.savefig(os.path.join(results_dir, 'reconstruction_loss_validation_data.png'))\n",
        "    plt.close()\n",
        "\n",
        "    \"\"\"### **Mean and standard deviation of reconstruction losses for normal and abnormal test data\"\"\"\n",
        "    reconstructions = variational_ae.predict(normal_test_data)\n",
        "    nl_test_loss = tf.keras.losses.mae(reconstructions.reshape(-1,3072), normal_test_data.reshape(-1,3072))\n",
        "    np.mean(nl_test_loss) , np.std(nl_test_loss)\n",
        "\n",
        "    reconstructions = variational_ae.predict(abnormal_test_data)\n",
        "    abn_test_loss = tf.keras.losses.mae(reconstructions.reshape(-1,3072), abnormal_test_data.reshape(-1,3072))\n",
        "    np.mean(abn_test_loss) , np.std(abn_test_loss)\n",
        "\n",
        "    \"\"\"### **Calculation of the accuracy and the confusion matrix on the test data with threshold set based on the best threshold from the validation data**\"\"\"\n",
        "\n",
        "    def print_stats(predictions, labels):\n",
        "      cf = confusion_matrix(labels, predictions)\n",
        "      print(\"Confusion Matrix: \\n prediction: F      T \")\n",
        "      print(\"             {}   {}\".format(preds[preds == False].shape[0], preds[preds == True].shape[0]))\n",
        "      print(\" label: F   [[{}   {}]    {}\".format(cf[0,0], cf[0,1], test_labels_T_F[test_labels_T_F == False].shape[0]))\n",
        "      print(\"        T    [{}   {}]]   {}\".format(cf[1,0], cf[1,1], test_labels_T_F[test_labels_T_F == True].shape[0]))\n",
        "      accuracy = accuracy_score(labels, predictions)\n",
        "      print(\"Accuracy = {}\".format(accuracy))\n",
        "      normal_test_mean = np.mean(nl_test_loss)\n",
        "      print(\"Normal Test Data Mean = {}\".format(normal_test_mean))\n",
        "      normal_test_stdev = np.std(nl_test_loss)\n",
        "      print(\"Normal Test Data Standard Deviation = {}\".format(normal_test_stdev))\n",
        "      abnormal_test_mean = np.mean(abn_test_loss)\n",
        "      print(\"Abnormal Test Data Mean = {}\".format(abnormal_test_mean))\n",
        "      abnormal_test_stdev = np.std(abn_test_loss)\n",
        "      print(\"Abnormal Test Data Standard Deviation = {}\".format(abnormal_test_stdev))\n",
        "      precision = precision_score(labels, predictions)\n",
        "      print(\"Precision = {}\".format(precision))\n",
        "      recall = recall_score(labels, predictions)\n",
        "      print(\"Recall = {}\".format(recall))\n",
        "      return accuracy, normal_test_mean, normal_test_stdev, abnormal_test_mean, abnormal_test_stdev, precision, recall\n",
        "\n",
        "    preds = predict(variational_ae, test_data, threshold)\n",
        "    stats = print_stats(preds, test_labels_T_F)\n",
        "\n",
        "    print(\"Threshold =\", valid_data_best_threshold)\n",
        "\n",
        "    print(confusion_matrix(test_labels_T_F, preds))\n",
        "\n",
        "    # return only the item we need\n",
        "\n",
        "    \"\"\"#### **Extra accuracy info**\n",
        "    **Just informative. Please record the above accuracy.**\n",
        "\n",
        "    #### Accuracy on the test data with threshold set based on (threshold2 + threshold3) / 2 = Average of (mean + std of the distribution of the reconstruction losses of the normal validation data) and (mean - std of the distribution of the reconstruction losses of the abnormal validation data)\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"Extra results with different thresholds\")\n",
        "\n",
        "    preds = predict(variational_ae, test_data, Avg_of_threshold_2_3)\n",
        "    print_stats(preds, test_labels_T_F)\n",
        "\n",
        "    \"\"\"#### Accuracy on the test data with threshold set based on the mean of the training data MAE reconstruction losses + 2.5 std\"\"\"\n",
        "\n",
        "    preds = predict(variational_ae, test_data, threshold_train_mean_2_5_std)\n",
        "    print_stats(preds, test_labels_T_F)\n",
        "\n",
        "    return stats"
      ],
      "metadata": {
        "id": "2okVUk1xS7eq"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_experiments(normals, abnormals):\n",
        "  dirname = 'normals=' + ','.join(map(str,normals)) + ',abnormals=' + ','.join(map(str, abnormals))\n",
        "  results_dir = os.path.join('VAE', dirname)\n",
        "  if not os.path.isdir(results_dir):\n",
        "    os.makedirs(results_dir)\n",
        "  filename = os.path.join(results_dir, 'results.csv')\n",
        "  res = np.empty([3,7])\n",
        "  for i in range(3):\n",
        "    print(i+1, 'out of', 3)\n",
        "    loop_dir = os.path.join(results_dir, str(i))\n",
        "    if not os.path.isdir(loop_dir):\n",
        "      os.makedirs(loop_dir)\n",
        "    res[i] = test_and_train_model(normals, abnormals, loop_dir)\n",
        "  np.savetxt(filename, res, delimiter=',')\n",
        "  return res"
      ],
      "metadata": {
        "id": "F815eXkJaJt-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normals = [0] # airplane\n",
        "abnormals = [3] # cat\n",
        "res1 = run_experiments(normals, abnormals)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "q3DRXJ8gaOHU",
        "outputId": "410dc24d-0b97-4f98-d418-b5a6b010366e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 out of 3\n",
            "Epoch 1/100\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.6997INFO:tensorflow:Assets written to: wo_latent_VAE_model/assets\n",
            "36/36 [==============================] - 71s 2s/step - loss: 0.6997 - val_loss: 0.6877\n",
            "Epoch 2/100\n",
            "34/36 [===========================>..] - ETA: 3s - loss: 0.6784"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-928e07d6a74f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnormals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# airplane\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mabnormals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# cat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mres1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_experiments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mabnormals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-6-503e178d1f95>\u001b[0m in \u001b[0;36mrun_experiments\u001b[0;34m(normals, abnormals)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloop_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m       \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloop_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_and_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mabnormals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloop_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m   \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavetxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-a52c56ee3894>\u001b[0m in \u001b[0;36mtest_and_train_model\u001b[0;34m(normals, abnormals, results_dir)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     history = variational_ae.fit(normal_data, normal_data, epochs=100, batch_size=128, callbacks=[checkpoint_cb],\n\u001b[0;32m--> 144\u001b[0;31m                                 validation_data=(normal_valid_data, normal_valid_data), shuffle=True)\n\u001b[0m\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1382\u001b[0m                 _r=1):\n\u001b[1;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2956\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2957\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2959\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1852\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1853\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1854\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    502\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    505\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "normals = [5] # dog\n",
        "abnormals = [9] # truck\n",
        "res2 = run_experiments(normals, abnormals)"
      ],
      "metadata": {
        "id": "eVLdoMnzaQ8k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normals = [1,9] # truck, automobile\n",
        "abnormals = [3] # cat\n",
        "res4 = run_experiments(normals, abnormals)"
      ],
      "metadata": {
        "id": "Q9UKU4DJaTHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normals = [3,5] # cat, dog\n",
        "abnormals = [0] # airplane\n",
        "res5 = run_experiments(normals, abnormals)"
      ],
      "metadata": {
        "id": "6ZkYXqdlaVZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normals = [2,3,4,5,6,7] # animals\n",
        "abnormals = [0] # airplane\n",
        "res6 = run_experiments(normals, abnormals)"
      ],
      "metadata": {
        "id": "obJ02hDNaYIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normals = [0,1,8,9] # transportation\n",
        "abnormals = [2] # bird\n",
        "res7 = run_experiments(normals, abnormals)"
      ],
      "metadata": {
        "id": "N5Ku2F-Wae1y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}