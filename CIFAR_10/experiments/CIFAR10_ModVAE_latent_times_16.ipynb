{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CIFAR10_ModVAE_latent_times_16.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMGWPSIjkx4qqEndjylGfxd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mjhoover1/autoencoders/blob/main/CIFAR_10/experiments/CIFAR10_ModVAE_latent_times_16.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IrO11fiWSG5X"
      },
      "outputs": [],
      "source": [
        "import sklearn\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)\n",
        "import os\n",
        "from keras.layers import *\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_and_train_model(normals, abnormals, results_dir):\n",
        "    (x_train_0, y_train_0), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
        "\n",
        "    x_train_0 = x_train_0.astype(np.float32) / 255\n",
        "    x_test = x_test.astype(np.float32) / 255\n",
        "\n",
        "    train_size = x_train_0.shape[0] * 9 // 10\n",
        "\n",
        "    x_train, x_valid, y_train, y_valid = train_test_split(x_train_0, y_train_0, train_size = train_size)\n",
        "\n",
        "    normal_data = x_train[np.isin(y_train, normals).flatten()]    # Normal training data (Normal digits)\n",
        "    normal_labels = y_train[np.isin(y_train, normals).flatten()]  \n",
        "\n",
        "    valid_data = x_valid[np.isin(y_valid, abnormals).flatten() | np.isin(y_valid, normals).flatten()]    # Validation data (both normal digits and the abnormal digit)\n",
        "    valid_labels = y_valid[np.isin(y_valid, abnormals).flatten() | np.isin(y_valid, normals).flatten()]\n",
        "\n",
        "    test_data = x_test[np.isin(y_test, abnormals).flatten() | np.isin(y_test, normals).flatten()]   # Test data (both normal digits and the abnormal digit)\n",
        "    test_labels = y_test[np.isin(y_test, abnormals).flatten() | np.isin(y_test, normals).flatten()]\n",
        "\n",
        "    test_labels_T_F = np.where(np.isin(test_labels, normals).flatten(), True, False) \n",
        "    # Array of T and F, T where test digits are normal and F where test digits are abnormal\n",
        "\n",
        "    valid_labels_T_F = np.where(np.isin(valid_labels, normals).flatten(), True, False) \n",
        "    # Array of T and F, T where test digits are normal and F where test digits are abnormal\n",
        "\n",
        "    normal_data.shape, normal_labels.shape, valid_data.shape, valid_labels.shape, test_data.shape, test_labels.shape\n",
        "\n",
        "    normal_test_data = test_data[np.isin(test_labels, normals).flatten()]   # The normal digits in the test data\n",
        "    abnormal_test_data = test_data[np.isin(test_labels, abnormals).flatten()]                          # The abnormal digits in the test data\n",
        "    normal_test_labels = test_labels[np.isin(test_labels, normals).flatten()]   # Their labels\n",
        "    abnormal_test_labels = test_labels[np.isin(test_labels, abnormals).flatten()]                          # Their labels\n",
        "\n",
        "    normal_test_data.shape, abnormal_test_data.shape\n",
        "\n",
        "    normal_valid_data = valid_data[np.isin(valid_labels, normals).flatten()]   # The normal digits in the valid data\n",
        "    abnormal_valid_data = valid_data[np.isin(valid_labels, abnormals).flatten()]                           # The abnormal digits in the valid data\n",
        "    normal_valid_labels = valid_labels[np.isin(valid_labels, normals).flatten()]   # Their labels\n",
        "    abnormal_valid_labels = valid_labels[np.isin(valid_labels, abnormals).flatten()]                           # Their labels\n",
        "\n",
        "    normal_valid_data.shape, abnormal_valid_data.shape\n",
        "\n",
        "    \"\"\"### **Building and training the network** \"\"\"\n",
        "    K = keras.backend\n",
        "\n",
        "    # Modified sampling layer with the addition of mean_2, log_var_2, and fraction p, with\n",
        "    # the appropriate change in the reparametrization trick to do stochastic \n",
        "    # sampling from the superposition of the two MVN distributions, while allowing\n",
        "    # the 5 parallel layers containing the means and stds of the two MVNs and the fractions p's\n",
        "    # for each dimension to be trained via backpropogation of the error signal.\n",
        "    class Sampling(keras.layers.Layer):\n",
        "        def call(self, inputs):\n",
        "            mean_1, log_var_1, mean_2, log_var_2, p = inputs\n",
        "            return (K.random_normal(tf.shape(log_var_1)) * K.exp(log_var_1 / 2) + mean_1)*p + (K.random_normal(tf.shape(log_var_2)) * K.exp(log_var_2 / 2) + mean_2)*(1 - p)\n",
        "    \n",
        "    # For details please see Geron's book. \n",
        "    codings_size = 16   # The number of dimensions of the MVN distribution in the sampling layer\n",
        "\n",
        "    # inputs = keras.layers.Input(shape=[32, 32, 3])\n",
        "    # z = keras.layers.Flatten()(inputs)\n",
        "    # z = keras.layers.Dense(256, activation=\"selu\")(z)\n",
        "    # z = keras.layers.Dense(128, activation=\"selu\")(z)\n",
        "    # z = keras.layers.Dense(64, activation=\"selu\")(z)\n",
        "\n",
        "    inputs = keras.layers.Input(shape=[32, 32, 3])\n",
        "    # z = keras.layers.Flatten()(inputs)\n",
        "    # z = keras.layers.Dense(128, activation=\"selu\")(z)\n",
        "    #z = keras.layers.Reshape((257, 97, 1)), \n",
        "    z = Conv2D(64, (3, 3), padding='same')(inputs)\n",
        "    z = Conv2D(64, (3, 3), padding='same')(z)\n",
        "    # z = BatchNormalization(z)\n",
        "    z = Activation('relu')(z)\n",
        "    z = MaxPooling2D((2, 2), padding='same')(z)\n",
        "    z = Conv2D(32, (3, 3), padding='same')(z)\n",
        "    z = Conv2D(32, (3, 3), padding='same')(z)\n",
        "    # z = BatchNormalization(z)\n",
        "    z = Activation('relu')(z)\n",
        "    z = MaxPooling2D((2, 2), padding='same')(z)\n",
        "    z = keras.layers.Flatten()(z)\n",
        "\n",
        "    # Adding output nodes (parallel layers) at the end of the encoder for means \n",
        "    # and standard deviations of a second Multivariate Normal (MVN) distribution \n",
        "    # in the dimensions of the coding size (here 32). In each of the dimensions,\n",
        "    # this first MVN is multiplied by a fraction p and added to the second MVN\n",
        "    # multiplied by 1 - p in each dimension.\n",
        "    # final distribution = p * first MVN + (1 - p) * second MVN\n",
        "    # Another parallel layer (set of nodes) is added to keep and train the fractions p's\n",
        "    # in each dimension \n",
        "    codings_mean_1 = keras.layers.Dense(codings_size)(z)\n",
        "    codings_log_var_1 = keras.layers.Dense(codings_size)(z)\n",
        "    codings_mean_2 = keras.layers.Dense(codings_size)(z)\n",
        "    codings_log_var_2 = keras.layers.Dense(codings_size)(z)\n",
        "    codings_p = keras.layers.Dense(1, activation='sigmoid')(z)\n",
        "\n",
        "    # Modified sampling layer at the end of the encoder\n",
        "    codings = Sampling()([codings_mean_1, codings_log_var_1, codings_mean_2, codings_log_var_2, codings_p])\n",
        "    variational_encoder = keras.models.Model(\n",
        "        inputs=[inputs], outputs=[codings_mean_1, codings_log_var_1, codings_mean_2, codings_log_var_2, codings_p, codings])\n",
        "\n",
        "    decoder_inputs = keras.layers.Input(shape=[codings_size])\n",
        "\n",
        "    # tf.keras.layers.InputLayer(input_shape=(latent_dim,)),\n",
        "    #            tf.keras.layers.Dense(units=7*7*32, activation=tf.nn.relu),\n",
        "    #            tf.keras.layers.Reshape(target_shape=(7, 7, 32))\n",
        "\n",
        "    x = keras.layers.Dense(units=8*8*32, activation=\"relu\")(decoder_inputs)\n",
        "    x = keras.layers.Reshape(target_shape=(8, 8, 32))(x)\n",
        "    x = Conv2D(32, (3, 3), padding='same')(x)\n",
        "    x = Conv2D(32, (3, 3), padding='same')(x)\n",
        "    # x = BatchNormalization(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = UpSampling2D((2,2))(x)\n",
        "\n",
        "    x = Conv2D(64, (3, 3), padding='same')(x)\n",
        "    x = Conv2D(64, (3, 3), padding='same')(x)\n",
        "    # x = BatchNormalization(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = UpSampling2D((2,2))(x)\n",
        "    # x = keras.layers.Dense(128, activation=\"selu\")(x)\n",
        "    # x = keras.layers.Dense(32 * 32 * 3, activation=\"sigmoid\")(x)\n",
        "    x = Conv2D(3, (3, 3), padding='same')(x)\n",
        "    x = Conv2D(3, (3, 3), padding='same')(x)\n",
        "    # x = BatchNormalization(x)\n",
        "    x = Activation('sigmoid')(x)\n",
        "    outputs = keras.layers.Reshape([32, 32, 3])(x)\n",
        "\n",
        "    # x = keras.layers.Dense(64, activation=\"selu\")(decoder_inputs)\n",
        "    # x = keras.layers.Dense(128, activation=\"selu\")(x)\n",
        "    # x = keras.layers.Dense(256, activation=\"selu\")(x)\n",
        "    # x = keras.layers.Dense(32 * 32 * 3, activation=\"sigmoid\")(x)\n",
        "    # outputs = keras.layers.Reshape([32, 32, 3])(x)\n",
        "    variational_decoder = keras.models.Model(inputs=[decoder_inputs], outputs=[outputs])\n",
        "\n",
        "    _, _, _, _, _, codings = variational_encoder(inputs)\n",
        "    reconstructions = variational_decoder(codings)\n",
        "    variational_ae = keras.models.Model(inputs=[inputs], outputs=[reconstructions])\n",
        "\n",
        "    # New latent loss function that will be added to the reconstruction binary cross-entropy loss\n",
        "    # The whole network (Encoder, sampling layer, and decoder) will train to minimize this loss\n",
        "    p_mean = K.mean(codings_p)\n",
        "    array1 = p_mean*(codings_log_var_1 - K.exp(codings_log_var_1) - K.square(codings_mean_1))\n",
        "    array2 = (1-p_mean)*(codings_log_var_2 - K.exp(codings_log_var_2) - K.square(codings_mean_2))\n",
        "    sum1 = K.sum(1 + array1, axis=-1)\n",
        "    sum2 = K.sum(1 + array2, axis=-1)\n",
        "\n",
        "    latent_loss = -0.5 * (sum1 + sum2)\n",
        "\n",
        "    latent_loss = latent_loss * 16\n",
        "\n",
        "    # Add the latent loss to the reconstruction loss\n",
        "    variational_ae.add_loss(K.mean(latent_loss) / 784.)\n",
        "\n",
        "    # For the reconstruction loss binary cross-entropy loss is used (same as regular VAE). \n",
        "    # For details please see Chapter 17 of Geron's book (Stacked AE and VAE sections) \n",
        "    variational_ae.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\")\n",
        "\n",
        "    checkpoint_cb = keras.callbacks.ModelCheckpoint(\"modVAE_latent_times_16_model\", monitor=\"val_loss\", save_best_only=True)\n",
        "\n",
        "    history = variational_ae.fit(normal_data, normal_data, epochs=100, batch_size=128, callbacks=[checkpoint_cb],\n",
        "                                validation_data=(normal_valid_data, normal_valid_data), shuffle=True)\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "    plt.plot(history.history[\"loss\"], label=\"Training Loss\")\n",
        "    plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\n",
        "    plt.legend()\n",
        "    plt.savefig(os.path.join(results_dir, 'loss_history.png'))\n",
        "    plt.close()\n",
        "\n",
        "    model = variational_ae\n",
        "    model.summary(expand_nested=True, show_trainable=True)\n",
        "\n",
        "    model_encoder = variational_encoder\n",
        "    # model_encoder.summary(expand_nested=True, show_trainable=True)\n",
        "\n",
        "    model_decoder = variational_decoder\n",
        "    # model_decoder.summary(expand_nested=True, show_trainable=True)\n",
        "\n",
        "    model_layers = np.array(model.layers)\n",
        "    n_layers = model_layers.shape[0] \n",
        "    # np.concatenate((np.arange(n_layers).reshape(n_layers,1), model_layers.reshape(n_layers,1)), axis = 1)\n",
        "\n",
        "    \"\"\"### **The original and reconstructed images for the first 30 instances of the normal training data, validation data, normal validation data, abnormal validation data, test data, normal test data, and abnormal test data**\"\"\"\n",
        "\n",
        "    def plot_image(image):\n",
        "        plt.imshow(image, cmap=\"binary\")\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "    def show_reconstructions(model, images, n_images=5):\n",
        "        reconstructions = model.predict(images[:n_images])\n",
        "        fig = plt.figure(figsize=(n_images * 1.5, 3))\n",
        "        for image_index in range(n_images):\n",
        "            plt.subplot(2, n_images, 1 + image_index)\n",
        "            plot_image(images[image_index])\n",
        "            plt.subplot(2, n_images, 1 + n_images + image_index)\n",
        "            plot_image(reconstructions[image_index])\n",
        "\n",
        "    show_reconstructions(variational_ae, normal_data, 30)\n",
        "    plt.show()\n",
        "    plt.savefig(os.path.join(results_dir, 'reconstructions_normal.png'))\n",
        "    plt.close()\n",
        "\n",
        "    show_reconstructions(variational_ae, valid_data, 30)\n",
        "    plt.show()\n",
        "    plt.savefig(os.path.join(results_dir, 'reconstructions_valid.png'))\n",
        "    plt.close()\n",
        "\n",
        "    show_reconstructions(variational_ae, normal_valid_data, 30)\n",
        "    plt.show()\n",
        "    plt.savefig(os.path.join(results_dir, 'reconstructions_normal_valid.png'))\n",
        "    plt.close()\n",
        "\n",
        "    show_reconstructions(variational_ae, abnormal_valid_data, 30)\n",
        "    plt.show()\n",
        "    plt.savefig(os.path.join(results_dir, 'reconstructions_abnormal_valid.png'))\n",
        "    plt.close()\n",
        "\n",
        "    show_reconstructions(variational_ae, test_data, 30)\n",
        "    plt.show()\n",
        "    plt.savefig(os.path.join(results_dir, 'reconstructions_test.png'))\n",
        "    plt.close()\n",
        "\n",
        "    show_reconstructions(variational_ae, normal_test_data, 30)\n",
        "    plt.show()\n",
        "    plt.savefig(os.path.join(results_dir, 'reconstructions_normal_test.png'))\n",
        "    plt.close()\n",
        "\n",
        "    show_reconstructions(variational_ae, abnormal_test_data, 30)\n",
        "    plt.show()\n",
        "    plt.savefig(os.path.join(results_dir, 'reconstructions_abnormal_test.png'))\n",
        "    plt.close()\n",
        "\n",
        "    \"\"\"**1-Dim plot of pixels of the first normal test data**\"\"\"\n",
        "\n",
        "    reconstructions_nl_test = variational_ae.predict(normal_test_data)\n",
        "\n",
        "    plt.figure(figsize=(25,7))\n",
        "    plt.plot(normal_test_data[0].ravel(), 'r')\n",
        "    plt.plot(reconstructions_nl_test[0].ravel(), 'g')\n",
        "    plt.fill_between(np.arange(32*32*3), reconstructions_nl_test[0].ravel(), normal_test_data[0].ravel(), color='blue')\n",
        "    plt.legend(labels=[\"Input\", \"Reconstruction\", \"Error\"])\n",
        "    plt.show()\n",
        "    plt.savefig(os.path.join(results_dir, 'reconstruction_error_normal.png'))\n",
        "    plt.close()\n",
        "\n",
        "    \"\"\"**1-Dim plot of pixels of the first abnormal test data**\"\"\"\n",
        "\n",
        "    reconstructions_nl_test = variational_ae.predict(normal_test_data)\n",
        "    \n",
        "    plt.figure(figsize=(25,7))\n",
        "    plt.plot(normal_test_data[0].ravel(), 'r')\n",
        "    plt.plot(reconstructions_nl_test[0].ravel(), 'g')\n",
        "    plt.fill_between(np.arange(32*32*3), reconstructions_nl_test[0].ravel(), normal_test_data[0].ravel(), color='blue')\n",
        "    plt.legend(labels=[\"Input\", \"Reconstruction\", \"Error\"])\n",
        "    plt.show()\n",
        "    plt.savefig(os.path.join(results_dir, 'reconstruction_error_normal.png'))\n",
        "    plt.close()\n",
        "\n",
        "    \"\"\"### **Distributions of the reconstruction losses and the calculation of the threshold.**\n",
        "\n",
        "    **Distribution of the reconstruction losses of the normal training data**\n",
        "    \"\"\"\n",
        "    reconstructions = variational_ae.predict(normal_data)\n",
        "    train_loss = tf.keras.losses.mae(reconstructions.reshape(-1, 3072), normal_data.reshape(-1, 3072))\n",
        "    plt.figure(figsize=(10,5))\n",
        "    plt.hist(train_loss[None,:], bins=100)\n",
        "    threshold1 = np.mean(train_loss) + 2.5*np.std(train_loss)\n",
        "    plt.axvline(threshold1,c='g')\n",
        "    plt.xlabel(\"MAE reconstruction loss of the normal training data\")\n",
        "    plt.ylabel(\"No of examples\")\n",
        "    plt.show()\n",
        "    plt.savefig(os.path.join(results_dir, 'reconstruction_losses_normal.png'))\n",
        "    plt.close()\n",
        "\n",
        "    print(\"Mean: \", np.mean(train_loss))\n",
        "    print(\"Std: \", np.std(train_loss))\n",
        "\n",
        "    threshold_train_mean_2_5_std = np.mean(train_loss) + 2.5*np.std(train_loss)\n",
        "    print(\"Threshold based on the mean of the training data MAE reconstruction losses + 2.5 std: \", threshold_train_mean_2_5_std)\n",
        "\n",
        "    threshold1 = threshold_train_mean_2_5_std\n",
        "\n",
        "    \"\"\"**Distribution of the reconstruction losses of the abnormal validation data**\"\"\"\n",
        "    reconstructions = variational_ae.predict(abnormal_valid_data)\n",
        "    abn_valid_loss = tf.keras.losses.mae(reconstructions.reshape(-1,3072), abnormal_valid_data.reshape(-1,3072))\n",
        "    plt.figure(figsize=(10,5))\n",
        "    plt.hist(abn_valid_loss[None, :], bins=100)\n",
        "    threshold2 = np.mean(abn_valid_loss) - np.std(abn_valid_loss)\n",
        "    plt.axvline(threshold2,c='cyan')\n",
        "    plt.axvline(threshold1,c='g')\n",
        "    plt.xlabel(\"MAE reconstruction loss of the abnormal validation data\")\n",
        "    plt.ylabel(\"No of examples\")\n",
        "    plt.show()\n",
        "    plt.savefig(os.path.join(results_dir, 'reconstruction_loss_abnormal_validation_data.png'))\n",
        "    plt.close()\n",
        "\n",
        "    abnormal_valid_mean_loss = np.mean(abn_valid_loss)\n",
        "\n",
        "    abnormal_valid_mean_loss , np.std(abn_valid_loss)\n",
        "\n",
        "    threshold2 = abnormal_valid_mean_loss - np.std(abn_valid_loss)\n",
        "    print(\"Threshold2: \", threshold2)\n",
        "\n",
        "    \"\"\"**Distribution of the reconstruction losses of the normal validation data**\"\"\"\n",
        "\n",
        "    reconstructions = variational_ae.predict(normal_valid_data)\n",
        "    nl_valid_loss = tf.keras.losses.mae(reconstructions.reshape(-1,3072), normal_valid_data.reshape(-1,3072))\n",
        "    plt.figure(figsize=(10,5))\n",
        "    plt.hist(nl_valid_loss[None, :], bins=100)\n",
        "    threshold3 = np.mean(nl_valid_loss) + np.std(nl_valid_loss)\n",
        "    plt.axvline(threshold3, c='magenta')\n",
        "    plt.axvline(threshold2, c='cyan')\n",
        "    plt.axvline(threshold1, c='g')\n",
        "    plt.xlabel(\"MAE reconstruction loss of the normal validation data\")\n",
        "    plt.ylabel(\"No of examples\")\n",
        "    plt.show()\n",
        "\n",
        "    normal_valid_mean_loss = np.mean(nl_valid_loss)\n",
        "    normal_valid_mean_loss , np.std(nl_valid_loss)\n",
        "\n",
        "    threshold3 = normal_valid_mean_loss + np.std(nl_valid_loss)\n",
        "    print(\"Threshold3: \", threshold3)\n",
        "\n",
        "    \"\"\"**Calculation of a preliminary threshold based on (threshold2 + threshold3) / 2 = Average of (mean + std of the distribution of the reconstruction losses of the normal validation data) and (mean - std of the distribution of the reconstruction losses of the abnormal validation data)**\"\"\"\n",
        "\n",
        "    Avg_of_threshold_2_3 = (threshold2 + threshold3)/2\n",
        "    print(\"Average of threshold 2 and 3: \", Avg_of_threshold_2_3)\n",
        "\n",
        "    threshold4 = Avg_of_threshold_2_3\n",
        "\n",
        "    \"\"\"### **Calculation of the threshold that gives the best accuracy on the validation data and set this as the threshold.**\"\"\"\n",
        "\n",
        "    def predict(model, data, threshold):\n",
        "      reconstructions = model.predict(data)\n",
        "      loss = tf.keras.losses.mae(reconstructions.reshape(-1, 3072), data.reshape(-1, 3072))\n",
        "      return tf.math.less(loss, threshold)\n",
        "\n",
        "    increment = (abnormal_valid_mean_loss- normal_valid_mean_loss)/100\n",
        "    thresholds = np.arange(normal_valid_mean_loss, abnormal_valid_mean_loss, increment)\n",
        "    thrs_size = thresholds.shape[0]\n",
        "    accuracies = np.zeros(thrs_size)\n",
        "    for i in range(thrs_size):\n",
        "      preds = predict(variational_ae, valid_data, thresholds[i])\n",
        "      accuracies[i] = accuracy_score(preds, valid_labels_T_F)\n",
        "    argmax = np.argmax(accuracies)\n",
        "    valid_data_best_threshold = thresholds[argmax]\n",
        "    print(\"The best threshold based on validation data: \", valid_data_best_threshold)\n",
        "\n",
        "    thr_acc = np.zeros((thrs_size, 2))\n",
        "    thr_acc[:, 0] = thresholds\n",
        "    thr_acc[:, 1] = accuracies\n",
        "    thr_acc[argmax-2:argmax+3]\n",
        "\n",
        "    threshold5 = valid_data_best_threshold\n",
        "\n",
        "    threshold = threshold5\n",
        "\n",
        "    \"\"\"#### **Distribution of the reconstruction losses of all the validation data (normal and abnormal)**\n",
        "\n",
        "    The blue line is threshold4 (= the average of threshold3 [magenta] and threshold2 [cyan]). \n",
        "\n",
        "    The red line is the threshold that gives the best accuracy for the validation data.\n",
        "    \"\"\"\n",
        "\n",
        "    reconstructions = variational_ae.predict(valid_data)\n",
        "    valid_loss = tf.keras.losses.mae(reconstructions.reshape(-1,3072), valid_data.reshape(-1,3072))\n",
        "\n",
        "    \n",
        "    reconstructions = variational_ae.predict(valid_data)\n",
        "    valid_loss = tf.keras.losses.mae(reconstructions.reshape(-1,3072), valid_data.reshape(-1,3072))\n",
        "    plt.figure(figsize=(20,5))\n",
        "    plt.hist(valid_loss[None, :], bins=100)\n",
        "    plt.axvline(threshold, c='r')\n",
        "    plt.axvline(threshold4, c='b')\n",
        "    plt.axvline(threshold2, c='cyan')\n",
        "    plt.axvline(threshold3, c='magenta')\n",
        "    plt.axvline(threshold1, c='green')\n",
        "    plt.xlabel(\"MAE reconstruction loss of the validation data\")\n",
        "    plt.ylabel(\"No of examples\")\n",
        "    plt.show()\n",
        "    plt.savefig(os.path.join(results_dir, 'reconstruction_loss_validation_data.png'))\n",
        "    plt.close()\n",
        "\n",
        "    \"\"\"### **Mean and standard deviation of reconstruction losses for normal and abnormal test data\"\"\"\n",
        "    reconstructions = variational_ae.predict(normal_test_data)\n",
        "    nl_test_loss = tf.keras.losses.mae(reconstructions.reshape(-1,3072), normal_test_data.reshape(-1,3072))\n",
        "    np.mean(nl_test_loss) , np.std(nl_test_loss)\n",
        "\n",
        "    reconstructions = variational_ae.predict(abnormal_test_data)\n",
        "    abn_test_loss = tf.keras.losses.mae(reconstructions.reshape(-1,3072), abnormal_test_data.reshape(-1,3072))\n",
        "    np.mean(abn_test_loss) , np.std(abn_test_loss)\n",
        "\n",
        "    \"\"\"### **Calculation of the accuracy and the confusion matrix on the test data with threshold set based on the best threshold from the validation data**\"\"\"\n",
        "\n",
        "    def print_stats(predictions, labels):\n",
        "      cf = confusion_matrix(labels, predictions)\n",
        "      print(\"Confusion Matrix: \\n prediction: F      T \")\n",
        "      print(\"             {}   {}\".format(preds[preds == False].shape[0], preds[preds == True].shape[0]))\n",
        "      print(\" label: F   [[{}   {}]    {}\".format(cf[0,0], cf[0,1], test_labels_T_F[test_labels_T_F == False].shape[0]))\n",
        "      print(\"        T    [{}   {}]]   {}\".format(cf[1,0], cf[1,1], test_labels_T_F[test_labels_T_F == True].shape[0]))\n",
        "      accuracy = accuracy_score(labels, predictions)\n",
        "      print(\"Accuracy = {}\".format(accuracy))\n",
        "      normal_test_mean = np.mean(nl_test_loss)\n",
        "      print(\"Normal Test Data Mean = {}\".format(normal_test_mean))\n",
        "      normal_test_stdev = np.std(nl_test_loss)\n",
        "      print(\"Normal Test Data Standard Deviation = {}\".format(normal_test_stdev))\n",
        "      abnormal_test_mean = np.mean(abn_test_loss)\n",
        "      print(\"Abnormal Test Data Mean = {}\".format(abnormal_test_mean))\n",
        "      abnormal_test_stdev = np.std(abn_test_loss)\n",
        "      print(\"Abnormal Test Data Standard Deviation = {}\".format(abnormal_test_stdev))\n",
        "      precision = precision_score(labels, predictions)\n",
        "      print(\"Precision = {}\".format(precision))\n",
        "      recall = recall_score(labels, predictions)\n",
        "      print(\"Recall = {}\".format(recall))\n",
        "      return accuracy, normal_test_mean, normal_test_stdev, abnormal_test_mean, abnormal_test_stdev, precision, recall\n",
        "\n",
        "    preds = predict(variational_ae, test_data, threshold)\n",
        "    stats = print_stats(preds, test_labels_T_F)\n",
        "\n",
        "    print(\"Threshold =\", valid_data_best_threshold)\n",
        "\n",
        "    print(confusion_matrix(test_labels_T_F, preds))\n",
        "\n",
        "    # return only the item we need\n",
        "\n",
        "    \"\"\"#### **Extra accuracy info**\n",
        "    **Just informative. Please record the above accuracy.**\n",
        "\n",
        "    #### Accuracy on the test data with threshold set based on (threshold2 + threshold3) / 2 = Average of (mean + std of the distribution of the reconstruction losses of the normal validation data) and (mean - std of the distribution of the reconstruction losses of the abnormal validation data)\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"Extra results with different thresholds\")\n",
        "\n",
        "    preds = predict(variational_ae, test_data, Avg_of_threshold_2_3)\n",
        "    print_stats(preds, test_labels_T_F)\n",
        "\n",
        "    \"\"\"#### Accuracy on the test data with threshold set based on the mean of the training data MAE reconstruction losses + 2.5 std\"\"\"\n",
        "\n",
        "    preds = predict(variational_ae, test_data, threshold_train_mean_2_5_std)\n",
        "    print_stats(preds, test_labels_T_F)\n",
        "\n",
        "    return stats"
      ],
      "metadata": {
        "id": "2okVUk1xS7eq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_experiments(normals, abnormals):\n",
        "  dirname = 'normals=' + ','.join(map(str,normals)) + ',abnormals=' + ','.join(map(str, abnormals))\n",
        "  results_dir = os.path.join('VAE', dirname)\n",
        "  if not os.path.isdir(results_dir):\n",
        "    os.makedirs(results_dir)\n",
        "  filename = os.path.join(results_dir, 'results.csv')\n",
        "  res = np.empty([3,7])\n",
        "  for i in range(3):\n",
        "    print(i+1, 'out of', 3)\n",
        "    loop_dir = os.path.join(results_dir, str(i))\n",
        "    if not os.path.isdir(loop_dir):\n",
        "      os.makedirs(loop_dir)\n",
        "    res[i] = test_and_train_model(normals, abnormals, loop_dir)\n",
        "  np.savetxt(filename, res, delimiter=',')\n",
        "  return res"
      ],
      "metadata": {
        "id": "gc4vp1q-TAQU"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normals = [0] # airplane\n",
        "abnormals = [3] # cat\n",
        "res1 = run_experiments(normals, abnormals)"
      ],
      "metadata": {
        "id": "GDjgMWisVDuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normals = [5] # dog\n",
        "abnormals = [9] # truck\n",
        "res2 = run_experiments(normals, abnormals)"
      ],
      "metadata": {
        "id": "9lwG13F0VfxS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normals = [6] # frog\n",
        "abnormals = [8] # ship\n",
        "res3 = run_experiments(normals, abnormals)"
      ],
      "metadata": {
        "id": "96FvK0loVlpo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normals = [1,9] # truck, automobile\n",
        "abnormals = [3] # cat\n",
        "res4 = run_experiments(normals, abnormals)"
      ],
      "metadata": {
        "id": "wPmlXxkBVmhu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normals = [3,5] # cat, dog\n",
        "abnormals = [0] # airplane\n",
        "res5 = run_experiments(normals, abnormals)"
      ],
      "metadata": {
        "id": "myVRq1XdVsaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normals = [2,3,4,5,6,7] # animals\n",
        "abnormals = [0] # airplane\n",
        "res6 = run_experiments(normals, abnormals)"
      ],
      "metadata": {
        "id": "V_S4A8GnVvoP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normals = [0,1,8,9] # transportation\n",
        "abnormals = [2] # bird\n",
        "res7 = run_experiments(normals, abnormals)"
      ],
      "metadata": {
        "id": "x6XFojM1V64B"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}